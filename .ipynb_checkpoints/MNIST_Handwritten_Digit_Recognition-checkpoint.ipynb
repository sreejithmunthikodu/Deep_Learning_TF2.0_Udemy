{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 22:33:05.076838 140061408048960 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=1.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit dataset into train and test\n",
    "x_train, x_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
    "# define size of cross validation dataset\n",
    "n_cross_validation = 0.1*mnist_info.splits[\"train\"].num_examples\n",
    "n_cross_validation = tf.cast(n_cross_validation, tf.int64)\n",
    "\n",
    "# define size of test dataset\n",
    "n_test = mnist_info.splits[\"test\"].num_examples\n",
    "n_test = tf.cast(n_test, tf.int64)\n",
    "\n",
    "# define a function to standardize the values\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "# Apply dataset.map() function to transform the data\n",
    "scaled_train_valid_data = x_train.map(scale)\n",
    "scaled_test_data = x_test.map(scale)\n",
    "\n",
    "# Shuffle the training dataset\n",
    "buffer_size = 10000\n",
    "\n",
    "# Shuffle train and validation datasest\n",
    "shuffled_train_validation_data = scaled_train_valid_data.shuffle(buffer_size)\n",
    "# Select first n_cross_validation samples as cross_validation dataset\n",
    "validation_data = shuffled_train_validation_data.take(n_cross_validation)\n",
    "# Slip first n_cross_validation samples to form the train_data\n",
    "train_data = shuffled_train_validation_data.skip(n_cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up batches for sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "# define training batches of batch size 100\n",
    "train_data = train_data.batch(batch_size)\n",
    "# define a single batch of all validation data\n",
    "validation_data = validation_data.batch(n_cross_validation)\n",
    "# define a single batch of all test data\n",
    "test_data = scaled_test_data.batch(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into input data and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = next(iter(train_data))\n",
    "x_valid, y_valid = next(iter(validation_data))\n",
    "x_test, y_test = next(iter(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 - 9s - loss: 0.6488 - accuracy: 0.8154 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "54/54 - 5s - loss: 0.1912 - accuracy: 0.9433 - val_loss: 0.1609 - val_accuracy: 0.9560\n",
      "Epoch 3/10\n",
      "54/54 - 5s - loss: 0.1258 - accuracy: 0.9631 - val_loss: 0.1117 - val_accuracy: 0.9692\n",
      "Epoch 4/10\n",
      "54/54 - 5s - loss: 0.0973 - accuracy: 0.9703 - val_loss: 0.0914 - val_accuracy: 0.9733\n",
      "Epoch 5/10\n",
      "54/54 - 8s - loss: 0.0749 - accuracy: 0.9773 - val_loss: 0.0739 - val_accuracy: 0.9797\n",
      "Epoch 6/10\n",
      "54/54 - 5s - loss: 0.0572 - accuracy: 0.9829 - val_loss: 0.0631 - val_accuracy: 0.9822\n",
      "Epoch 7/10\n",
      "54/54 - 5s - loss: 0.0482 - accuracy: 0.9856 - val_loss: 0.0548 - val_accuracy: 0.9853\n",
      "Epoch 8/10\n",
      "54/54 - 5s - loss: 0.0423 - accuracy: 0.9867 - val_loss: 0.0449 - val_accuracy: 0.9875\n",
      "Epoch 9/10\n",
      "54/54 - 5s - loss: 0.0365 - accuracy: 0.9884 - val_loss: 0.0408 - val_accuracy: 0.9887\n",
      "Epoch 10/10\n",
      "54/54 - 6s - loss: 0.0243 - accuracy: 0.9929 - val_loss: 0.0511 - val_accuracy: 0.9863\n",
      "CPU times: user 5min 54s, sys: 1min 15s, total: 7min 9s\n",
      "Wall time: 57.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f61601233c8>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "NUM_EPOCH = 10\n",
    "model.fit(train_data, epochs=NUM_EPOCH, validation_data=(x_valid, y_valid), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 856ms/step - loss: 0.0746 - accuracy: 0.9778"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07455597817897797, 0.9778]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2-gpu]",
   "language": "python",
   "name": "conda-env-tf2-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
